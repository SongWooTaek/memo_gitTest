# Regression analysis(회귀분석)


library(boot)
data(nodal)
a <- c(2,4,6,7)
data <- nodal[,a]
glmModel <- glm(r~., data = data, family = "binomial") # glm() -> 회귀분석함수
summary(glmModel)

## 최적회귀방정식
# 단계적 변수선택
# 1. 전진선택법(forward slection)
# 의미: 절편만 있는 상수모형으로부터 시작해 중료하다고 생각되는 설명변수부터 차례로 모형에 추가
# 단점: 변수값의 작은 변동에도 그 결과가 크게 달라져 안정성이 부족하다는 단점

# 2. 후진제거법(backward elimination)
# 의미: 독립변수 후보 모두를 포함한 모형에서 출발해 가장 적은 영향을 주는 변수부터 하나씩 제거하면서 
#       더 이상 제거할 변수가 없을 때의 모형을 선택
# 단점: 변수의 개수가 많은 경우 사용하기 어려움

# 3. 단계선택법(stepwise method)
# 의미: 전진선택법에 의해 변수를 추가하면서 새롭게 추가된 변수에 기인해 기존 변수의 중요도가 약화되면
#       해당변수를 제거하는 등 단계별로 추가 또는 제거되는 변수 여부를 검토해 더 이상 없을 때 중단

## 최적회귀방정식 사례

# 1.데이터 프레임 생성

x1 <- c(7,1,11,11,7,11,3,1,2,21,1,11,10)
x2 <- c(26,29,56,31,52,55,71,31,54,47,40,66,68)
x3 <- c(6,15,8,8,6,9,17,22,18,4,23,9,8)
x4 <- c(60,52,20,47,33,22,6,44,22,26,34,12,12)
y <- c(78.5,74.3,104.3,87.6,95.9,109.2,102.7,72.5,93.1,115.9,83.8,113.3,109.4)

df <- data.frame(x1,x2,x3,x4,y)
df

head(df)

a <- lm(y ~ x1 + x2 + x3 + x4, data = df)
summary(a)

b <- lm(y ~ x1 + x2 + x4, data = df)
summary(b)

c <- lm(y ~ x1 + x2, data = df)
summary(c)

## 변수 선택법 예제(별점화 전진선택법)

step(lm(y ~ 1, data = df), scope = list(lower = ~1, upper = ~x1+x2+x3+x4), direction = "forward")

# <결과>
# Start:  AIC=71.44
# y ~ 1
# 
# Df Sum of Sq     RSS    AIC
# + x4    1   1831.90  883.87 58.852 => 가장 낮은 값
# + x2    1   1809.43  906.34 59.178
# + x1    1   1450.08 1265.69 63.519
# + x3    1    776.36 1939.40 69.067
# <none>              2715.76 71.444
# 
# Step:  AIC=58.85
# y ~ x4
# 
# Df Sum of Sq    RSS    AIC
# + x1    1    809.10  74.76 28.742 => 가장 낮은 값
# + x3    1    708.13 175.74 39.853
# <none>              883.87 58.852
# + x2    1     14.99 868.88 60.629
# 
# Step:  AIC=28.74
# y ~ x4 + x1
# 
# Df Sum of Sq    RSS    AIC
# + x2    1    26.789 47.973 24.974 => 가장 낮은 값
# + x3    1    23.926 50.836 25.728
# <none>              74.762 28.742
# 
# Step:  AIC=24.97
# y ~ x4 + x1 + x2
# 
# Df Sum of Sq    RSS    AIC
# <none>              47.973 24.974 => 가장 낮은 값
# + x3    1   0.10909 47.864 26.944
# 
# Call:
#   lm(formula = y ~ x4 + x1 + x2, data = df)
# 
# Coefficients:
#   (Intercept)           x4           x1           x2  
# 71.6483      -0.2365       1.4519       0.4161 


# =>회귀식 : y = 71.6483 + -0.2365x4 + 1.4519x1 + 0.4161x2
















